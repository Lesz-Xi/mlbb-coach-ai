#!/usr/bin/env python3
"""
Hero Rule Autogenerator for MLBB Coach AI

This script creates a new hero evaluation module by:
1. Creating a new Python file in the appropriate role directory
2. Generating hero-specific evaluation logic based on role templates
3. Adding the hero to the thresholds.yml configuration
4. Creating a basic test file for the new hero

Usage:
    python scripts/scaffold_hero.py <hero_name> <role>

Example:
    python scripts/scaffold_hero.py beatrix marksman
    python scripts/scaffold_hero.py johnson tank
"""

import argparse
import sys
from pathlib import Path
import yaml

# Add the project root to the path so we can import from core
sys.path.insert(0, str(Path(__file__).parent.parent))

VALID_ROLES = ["marksman", "assassin", "mage", "tank", "support", "fighter"]

ROLE_TEMPLATES = {
    "marksman": '''"""
{hero_name} - Marksman Hero Evaluation
Auto-generated by scaffold_hero.py
"""

from core.base_evaluator import BaseEvaluator
from typing import Dict, Any, List, Tuple


class {hero_class}Evaluator(BaseEvaluator):
    """
    Evaluation logic for {hero_name} (Marksman)
    """
    
    def _evaluate_hero_specific(self, data: Dict[str, Any], 
                               thresholds: Dict, hero: str) -> List[Tuple[str, str]]:
        """
        {hero_name}-specific evaluation logic
        """
        fb = []
        
        # Example: Marksman-specific checks
        # TODO: Add {hero_name}-specific evaluation logic here
        
        return fb


def evaluate(data: Dict[str, Any], minutes: int = None) -> List[Tuple[str, str]]:
    """
    Main evaluation function for {hero_name}
    """
    evaluator = {hero_class}Evaluator()
    return evaluator.evaluate(data, minutes)
''',
    
    "assassin": '''"""
{hero_name} - Assassin Hero Evaluation
Auto-generated by scaffold_hero.py
"""

from core.base_evaluator import BaseEvaluator
from typing import Dict, Any, List, Tuple


class {hero_class}Evaluator(BaseEvaluator):
    """
    Evaluation logic for {hero_name} (Assassin)
    """
    
    def _evaluate_hero_specific(self, data: Dict[str, Any], 
                               thresholds: Dict, hero: str) -> List[Tuple[str, str]]:
        """
        {hero_name}-specific evaluation logic
        """
        fb = []
        
        # Example: Assassin-specific checks
        # TODO: Add {hero_name}-specific evaluation logic here
        
        return fb


def evaluate(data: Dict[str, Any], minutes: int = None) -> List[Tuple[str, str]]:
    """
    Main evaluation function for {hero_name}
    """
    evaluator = {hero_class}Evaluator()
    return evaluator.evaluate(data, minutes)
''',
    
    "mage": '''"""
{hero_name} - Mage Hero Evaluation
Auto-generated by scaffold_hero.py
"""

from core.base_evaluator import BaseEvaluator
from typing import Dict, Any, List, Tuple


class {hero_class}Evaluator(BaseEvaluator):
    """
    Evaluation logic for {hero_name} (Mage)
    """
    
    def _evaluate_hero_specific(self, data: Dict[str, Any], 
                               thresholds: Dict, hero: str) -> List[Tuple[str, str]]:
        """
        {hero_name}-specific evaluation logic
        """
        fb = []
        
        # Example: Mage-specific checks
        # TODO: Add {hero_name}-specific evaluation logic here
        
        return fb


def evaluate(data: Dict[str, Any], minutes: int = None) -> List[Tuple[str, str]]:
    """
    Main evaluation function for {hero_name}
    """
    evaluator = {hero_class}Evaluator()
    return evaluator.evaluate(data, minutes)
''',
    
    "tank": '''"""
{hero_name} - Tank Hero Evaluation
Auto-generated by scaffold_hero.py
"""

from core.base_evaluator import BaseEvaluator
from typing import Dict, Any, List, Tuple


class {hero_class}Evaluator(BaseEvaluator):
    """
    Evaluation logic for {hero_name} (Tank)
    """
    
    def _evaluate_hero_specific(self, data: Dict[str, Any], 
                               thresholds: Dict, hero: str) -> List[Tuple[str, str]]:
        """
        {hero_name}-specific evaluation logic
        """
        fb = []
        
        # Example: Tank-specific checks
        # TODO: Add {hero_name}-specific evaluation logic here
        
        return fb


def evaluate(data: Dict[str, Any], minutes: int = None) -> List[Tuple[str, str]]:
    """
    Main evaluation function for {hero_name}
    """
    evaluator = {hero_class}Evaluator()
    return evaluator.evaluate(data, minutes)
''',
    
    "support": '''"""
{hero_name} - Support Hero Evaluation
Auto-generated by scaffold_hero.py
"""

from core.base_evaluator import BaseEvaluator
from typing import Dict, Any, List, Tuple


class {hero_class}Evaluator(BaseEvaluator):
    """
    Evaluation logic for {hero_name} (Support)
    """
    
    def _evaluate_hero_specific(self, data: Dict[str, Any], 
                               thresholds: Dict, hero: str) -> List[Tuple[str, str]]:
        """
        {hero_name}-specific evaluation logic
        """
        fb = []
        
        # Example: Support-specific checks
        # TODO: Add {hero_name}-specific evaluation logic here
        
        return fb


def evaluate(data: Dict[str, Any], minutes: int = None) -> List[Tuple[str, str]]:
    """
    Main evaluation function for {hero_name}
    """
    evaluator = {hero_class}Evaluator()
    return evaluator.evaluate(data, minutes)
''',
    
    "fighter": '''"""
{hero_name} - Fighter Hero Evaluation
Auto-generated by scaffold_hero.py
"""

from core.base_evaluator import BaseEvaluator
from typing import Dict, Any, List, Tuple


class {hero_class}Evaluator(BaseEvaluator):
    """
    Evaluation logic for {hero_name} (Fighter)
    """
    
    def _evaluate_hero_specific(self, data: Dict[str, Any], 
                               thresholds: Dict, hero: str) -> List[Tuple[str, str]]:
        """
        {hero_name}-specific evaluation logic
        """
        fb = []
        
        # Example: Fighter-specific checks
        # TODO: Add {hero_name}-specific evaluation logic here
        
        return fb


def evaluate(data: Dict[str, Any], minutes: int = None) -> List[Tuple[str, str]]:
    """
    Main evaluation function for {hero_name}
    """
    evaluator = {hero_class}Evaluator()
    return evaluator.evaluate(data, minutes)
'''
}

TEST_TEMPLATE = '''"""
Test file for {hero_name} evaluation
Auto-generated by scaffold_hero.py
"""

import unittest
from typing import Dict, Any
from rules.roles.{role}.{hero_name_lower} import evaluate


class Test{hero_class}(unittest.TestCase):
    """Test cases for {hero_name} evaluation"""
    
    def setUp(self):
        """Set up test fixtures"""
        self.base_data = {{
            "hero": "{hero_name_lower}",
            "kills": 5,
            "deaths": 2,
            "assists": 8,
            "gold_per_min": 550,
            "hero_damage": 45000,
            "teamfight_participation": 65,
            "match_duration": 15
        }}
    
    def test_basic_evaluation(self):
        """Test basic evaluation with standard data"""
        result = evaluate(self.base_data, minutes=15)
        self.assertIsInstance(result, list)
        self.assertTrue(len(result) > 0)
        
        # Check that all results are tuples with severity and message
        for item in result:
            self.assertIsInstance(item, tuple)
            self.assertEqual(len(item), 2)
            self.assertIn(item[0], ["critical", "warning", "info", "success"])
    
    def test_high_performance(self):
        """Test evaluation with high performance data"""
        high_perf_data = self.base_data.copy()
        high_perf_data.update({{
            "kills": 10,
            "deaths": 1,
            "assists": 15,
            "gold_per_min": 700,
            "hero_damage": 80000
        }})
        
        result = evaluate(high_perf_data, minutes=15)
        self.assertIsInstance(result, list)
        
        # Should have some success messages for high performance
        success_messages = [msg for severity, msg in result if severity == "success"]
        self.assertTrue(len(success_messages) > 0)
    
    def test_low_performance(self):
        """Test evaluation with low performance data"""
        low_perf_data = self.base_data.copy()
        low_perf_data.update({{
            "kills": 1,
            "deaths": 8,
            "assists": 2,
            "gold_per_min": 300,
            "hero_damage": 15000
        }})
        
        result = evaluate(low_perf_data, minutes=15)
        self.assertIsInstance(result, list)
        
        # Should have warning or critical messages for low performance
        warning_messages = [msg for severity, msg in result 
                          if severity in ["warning", "critical"]]
        self.assertTrue(len(warning_messages) > 0)
    
    def test_missing_data(self):
        """Test evaluation with missing data"""
        incomplete_data = {{
            "hero": "{hero_name_lower}",
            "kills": 5,
            "deaths": 2,
            "assists": 8
        }}
        
        result = evaluate(incomplete_data, minutes=15)
        self.assertIsInstance(result, list)
        # Should still return some feedback even with missing data


if __name__ == "__main__":
    unittest.main()
'''


def create_hero_file(hero_name: str, role: str, project_root: Path) -> None:
    """Create the hero evaluation file"""
    hero_name_lower = hero_name.lower()
    hero_class = hero_name.capitalize()
    
    # Create the role directory if it doesn't exist
    role_dir = project_root / "rules" / "roles" / role
    role_dir.mkdir(parents=True, exist_ok=True)
    
    # Create the hero file
    hero_file = role_dir / f"{hero_name_lower}.py"
    
    if hero_file.exists():
        print(f"Warning: {hero_file} already exists. Skipping file creation.")
        return
    
    # Generate the hero code from template
    template = ROLE_TEMPLATES[role]
    hero_code = template.format(
        hero_name=hero_name,
        hero_name_lower=hero_name_lower,
        hero_class=hero_class,
        role=role
    )
    
    with open(hero_file, 'w') as f:
        f.write(hero_code)
    
    print(f"Created hero file: {hero_file}")


def create_test_file(hero_name: str, role: str, project_root: Path) -> None:
    """Create the test file for the hero"""
    hero_name_lower = hero_name.lower()
    hero_class = hero_name.capitalize()
    
    # Create the test file
    test_file = project_root / "tests" / f"test_{hero_name_lower}.py"
    
    if test_file.exists():
        print(f"Warning: {test_file} already exists. Skipping test file creation.")
        return
    
    # Generate the test code from template
    test_code = TEST_TEMPLATE.format(
        hero_name=hero_name,
        hero_name_lower=hero_name_lower,
        hero_class=hero_class,
        role=role
    )
    
    with open(test_file, 'w') as f:
        f.write(test_code)
    
    print(f"Created test file: {test_file}")


def update_thresholds_config(hero_name: str, role: str, project_root: Path) -> None:
    """Add the hero to the thresholds.yml configuration"""
    config_file = project_root / "config" / "thresholds.yml"
    
    if not config_file.exists():
        print(f"Warning: {config_file} not found. Cannot update thresholds.")
        return
    
    # Load existing configuration
    with open(config_file, 'r') as f:
        config = yaml.safe_load(f)
    
    hero_name_lower = hero_name.lower()
    
    # Check if hero already exists
    if 'heroes' not in config:
        config['heroes'] = {}
    
    if hero_name_lower in config['heroes']:
        print(f"Warning: {hero_name_lower} already exists in thresholds.yml. Skipping update.")
        return
    
    # Add hero with default role-based thresholds
    config['heroes'][hero_name_lower] = {
        f"# {hero_name} - {role.capitalize()} hero": None,
        "# TODO: Add hero-specific threshold overrides here": None
    }
    
    # Clean up None values (they're just for comments)
    config['heroes'][hero_name_lower] = {
        k: v for k, v in config['heroes'][hero_name_lower].items() 
        if v is not None
    }
    
    # If no specific overrides, just add a comment
    if not config['heroes'][hero_name_lower]:
        config['heroes'][hero_name_lower] = {
            "# Add hero-specific overrides here": "# e.g., damage_base: 3500"
        }
    
    # Write updated configuration
    with open(config_file, 'w') as f:
        yaml.dump(config, f, default_flow_style=False, sort_keys=False)
    
    print(f"Updated thresholds.yml with {hero_name_lower} configuration")


def main():
    """Main function"""
    parser = argparse.ArgumentParser(
        description="Generate a new hero evaluation module for MLBB Coach AI"
    )
    parser.add_argument("hero_name", help="Name of the hero (e.g., 'beatrix', 'johnson')")
    parser.add_argument("role", choices=VALID_ROLES, help="Role of the hero")
    parser.add_argument("--no-test", action="store_true", help="Skip creating test file")
    parser.add_argument("--no-config", action="store_true", help="Skip updating thresholds.yml")
    
    args = parser.parse_args()
    
    # Validate inputs
    hero_name = args.hero_name.lower().strip()
    role = args.role.lower().strip()
    
    if not hero_name:
        print("Error: Hero name cannot be empty")
        return 1
    
    if role not in VALID_ROLES:
        print(f"Error: Invalid role '{role}'. Must be one of: {', '.join(VALID_ROLES)}")
        return 1
    
    # Get project root
    project_root = Path(__file__).parent.parent
    
    print(f"Generating hero evaluation for: {hero_name.capitalize()} ({role})")
    print(f"Project root: {project_root}")
    print()
    
    try:
        # Create hero file
        create_hero_file(hero_name, role, project_root)
        
        # Create test file (unless skipped)
        if not args.no_test:
            create_test_file(hero_name, role, project_root)
        
        # Update configuration (unless skipped)
        if not args.no_config:
            update_thresholds_config(hero_name, role, project_root)
        
        print()
        print("Hero generation complete!")
        print(f"Next steps:")
        print(f"1. Edit rules/roles/{role}/{hero_name}.py to add {hero_name.capitalize()}-specific evaluation logic")
        print(f"2. Update config/thresholds.yml with {hero_name}-specific thresholds")
        print(f"3. Run tests/test_{hero_name}.py to verify the implementation")
        print(f"4. Test the hero with: python main.py --hero {hero_name}")
        
        return 0
        
    except Exception as e:
        print(f"Error: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())